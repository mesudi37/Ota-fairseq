{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fairseq-transliteration.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "R88wyQO2PPHE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# seq2seq with Fairseq\n",
        "\n",
        "This notebook uses Fairseq and PyTorch to train a sequence-to-sequence model.\n",
        "\n",
        "It clones and runs [github.com/deeplanguageclass/fairseq-transliteration/](https://github.com/deeplanguageclass/fairseq-transliteration/).\n",
        "\n",
        "The data are at [github.com/deeplanguageclass/fairseq-transliteration-data](https://github.com/deeplanguageclass/fairseq-transliteration-data).\n",
        "\n",
        "The notebook code itself is at [github.com/deeplanguageclass/fairseq-transliteration.ipynb](https://github.com/deeplanguageclass/fairseq-transliteration.ipynb).\n",
        "\n",
        "Note you must turn on GPU to use Fairseq!\n",
        "\n",
        "> *Edit > Notebook settings > Hardware accelerator: GPU*\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Yyowwr6LPX-z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ]
    },
    {
      "metadata": {
        "id": "PcyS5mCjPObY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "a95ee4cf-adb2-449a-8fd7-3893c6ed350a"
      },
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!rm -rf fairseq\n",
        "!git clone https://github.com/deeplanguageclass/fairseq.git\n",
        "%cd fairseq\n",
        "!ls\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'fairseq'...\n",
            "remote: Counting objects: 2224, done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 2224 (delta 23), reused 34 (delta 23), pack-reused 2189\u001b[K\n",
            "Receiving objects: 100% (2224/2224), 2.76 MiB | 6.93 MiB/s, done.\n",
            "Resolving deltas: 100% (1636/1636), done.\n",
            "/content/fairseq\n",
            "CONTRIBUTING.md       fairseq.gif\t\tPATENTS\t\t  scripts\n",
            "distributed_train.py  generate.py\t\tpreprocess.py\t  setup.py\n",
            "eval_lm.py\t      interactive.py\t\tREADME.md\t  tests\n",
            "examples\t      LICENSE\t\t\trequirements.txt  train.py\n",
            "fairseq\t\t      multiprocessing_train.py\tscore.py\n",
            "Collecting cffi (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/c0/47db8f624f3e4e2f3f27be03a93379d1ba16a1450a7b1aacfa0366e2c0dd/cffi-1.11.5-cp36-cp36m-manylinux1_x86_64.whl (421kB)\n",
            "\u001b[K    100% |████████████████████████████████| 430kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.14.5)\n",
            "Collecting torch (from -r requirements.txt (line 3))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 26kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x59ef2000 @  0x7f03b42bd1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hCollecting tqdm (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e6/19dfaff08fcbee7f3453e5b537e65a8364f1945f921a36d08be1e2ff3475/tqdm-4.24.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 10.1MB/s \n",
            "\u001b[?25hCollecting pycparser (from cffi->-r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/2d/aad7f16146f4197a11f8e91fb81df177adcc2073d36a17b1491fd09df6ed/pycparser-2.18.tar.gz (245kB)\n",
            "\u001b[K    100% |████████████████████████████████| 256kB 9.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pycparser\n",
            "  Running setup.py bdist_wheel for pycparser ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/c0/a1/27/5ba234bd77ea5a290cbf6d675259ec52293193467a12ef1f46\n",
            "Successfully built pycparser\n",
            "Installing collected packages: pycparser, cffi, torch, tqdm\n",
            "Successfully installed cffi-1.11.5 pycparser-2.18 torch-0.4.1 tqdm-4.24.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mdmKcCwFMqlV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2519
        },
        "outputId": "6f896a3f-cfa8-42a0-e38d-cdb54f5ad841"
      },
      "cell_type": "code",
      "source": [
        "!python setup.py build\n",
        "!python setup.py develop"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running build\r\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_label_smoothing.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_sequence_scorer.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_convtbc.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_average_checkpoints.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_sequence_generator.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_train.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_utils.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_binaries.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_dictionary.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/utils.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_data_utils.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/__init__.py -> build/lib.linux-x86_64-3.6/tests\n",
            "creating build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/options.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/trainer.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/distributed_utils.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/bleu.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/sequence_generator.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/meters.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/fp16_trainer.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/multiprocessing_pdb.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/sequence_scorer.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/utils.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/progress_bar.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/__init__.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "creating build/lib.linux-x86_64-3.6/scripts\n",
            "copying scripts/average_checkpoints.py -> build/lib.linux-x86_64-3.6/scripts\n",
            "copying scripts/build_sym_alignment.py -> build/lib.linux-x86_64-3.6/scripts\n",
            "copying scripts/__init__.py -> build/lib.linux-x86_64-3.6/scripts\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/language_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/fairseq_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/monolingual_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/dictionary.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/token_block_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/indexed_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/data_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/lstm.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/composite_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_incremental_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fconv_self_att.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fconv.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/beamable_mm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/downsampled_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/conv_tbc.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/linearized_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/grad_multiply.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/sinusoidal_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/scalar_bias.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/adaptive_softmax.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/learned_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/fairseq_task.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/language_modeling.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/fairseq_criterion.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/adaptive_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/label_smoothed_cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/sgd.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/nag.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/adagrad.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/adam.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/fairseq_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/inverse_square_root_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/fixed_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "running build_ext\n",
            "building 'fairseq.libbleu' extension\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/fairseq\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib/libbleu\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fdebug-prefix-map=/build/python3.6-sXpGnM/python3.6-3.6.3=. -specs=/usr/share/dpkg/no-pie-compile.specs -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o -std=c++11\n",
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fdebug-prefix-map=/build/python3.6-sXpGnM/python3.6-3.6.3=. -specs=/usr/share/dpkg/no-pie-compile.specs -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -std=c++11\n",
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -specs=/usr/share/dpkg/no-pie-link.specs -Wl,-z,relro -Wl,-Bsymbolic-functions -specs=/usr/share/dpkg/no-pie-link.specs -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.6-sXpGnM/python3.6-3.6.3=. -specs=/usr/share/dpkg/no-pie-compile.specs -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so\n",
            "running develop\n",
            "running egg_info\n",
            "creating fairseq.egg-info\n",
            "writing fairseq.egg-info/PKG-INFO\n",
            "writing dependency_links to fairseq.egg-info/dependency_links.txt\n",
            "writing requirements to fairseq.egg-info/requires.txt\n",
            "writing top-level names to fairseq.egg-info/top_level.txt\n",
            "writing manifest file 'fairseq.egg-info/SOURCES.txt'\n",
            "reading manifest file 'fairseq.egg-info/SOURCES.txt'\n",
            "writing manifest file 'fairseq.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "copying build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so -> fairseq\n",
            "Creating /usr/local/lib/python3.6/dist-packages/fairseq.egg-link (link to .)\n",
            "Adding fairseq 0.5.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/fairseq\n",
            "Processing dependencies for fairseq==0.5.0\n",
            "Searching for tqdm==4.24.0\n",
            "Best match: tqdm 4.24.0\n",
            "Adding tqdm 4.24.0 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for torch==0.4.1\n",
            "Best match: torch 0.4.1\n",
            "Adding torch 0.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.14.5\n",
            "Best match: numpy 1.14.5\n",
            "Adding numpy 1.14.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cffi==1.11.5\n",
            "Best match: cffi 1.11.5\n",
            "Adding cffi 1.11.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pycparser==2.18\n",
            "Best match: pycparser 2.18\n",
            "Adding pycparser 2.18 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for fairseq==0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3u_dRkrJO9fC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "4yDn86nIs2cr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1907
        },
        "outputId": "8684c5d4-ef10-4bad-ce4e-b2d6024892d5"
      },
      "cell_type": "code",
      "source": [
        "%cd examples/translation/\n",
        "!bash prepare-translit.sh\n",
        "%cd ../.."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fairseq/examples/translation\n",
            "Cloning Moses github repository (for tokenization scripts)...\n",
            "Cloning into 'mosesdecoder'...\n",
            "remote: Counting objects: 147104, done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 147104 (delta 0), reused 2 (delta 0), pack-reused 147098\u001b[K\n",
            "Receiving objects: 100% (147104/147104), 129.65 MiB | 22.00 MiB/s, done.\n",
            "Resolving deltas: 100% (113695/113695), done.\n",
            "Cloning Subword NMT repository (for BPE pre-processing)...\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Counting objects: 462, done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 462 (delta 20), reused 22 (delta 10), pack-reused 420\u001b[K\n",
            "Receiving objects: 100% (462/462), 208.23 KiB | 3.25 MiB/s, done.\n",
            "Resolving deltas: 100% (264/264), done.\n",
            "--2018-08-18 05:23:32--  https://deeplanguageclass.github.io/fairseq-transliteration-data/la-hy.train.tar.gz\n",
            "Resolving deeplanguageclass.github.io (deeplanguageclass.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to deeplanguageclass.github.io (deeplanguageclass.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24965008 (24M) [application/gzip]\n",
            "Saving to: ‘la-hy.train.tar.gz’\n",
            "\n",
            "la-hy.train.tar.gz  100%[===================>]  23.81M  33.0MB/s    in 0.7s    \n",
            "\n",
            "2018-08-18 05:23:33 (33.0 MB/s) - ‘la-hy.train.tar.gz’ saved [24965008/24965008]\n",
            "\n",
            "https://deeplanguageclass.github.io/fairseq-transliteration-data/la-hy.train.tar.gz successfully downloaded.\n",
            "train/\n",
            "train/translit.la-hy.la\n",
            "train/translit.la-hy.hy\n",
            "--2018-08-18 05:23:34--  https://deeplanguageclass.github.io/fairseq-transliteration-data/la-hy.test.tar.gz\n",
            "Resolving deeplanguageclass.github.io (deeplanguageclass.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to deeplanguageclass.github.io (deeplanguageclass.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 233679 (228K) [application/gzip]\n",
            "Saving to: ‘la-hy.test.tar.gz’\n",
            "\n",
            "la-hy.test.tar.gz   100%[===================>] 228.20K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2018-08-18 05:23:34 (5.33 MB/s) - ‘la-hy.test.tar.gz’ saved [233679/233679]\n",
            "\n",
            "https://deeplanguageclass.github.io/fairseq-transliteration-data/la-hy.test.tar.gz successfully downloaded.\n",
            "test/\n",
            "test/translit.la-hy.la\n",
            "test/translit.la-hy.hy\n",
            "pre-processing train data...\n",
            "rm: cannot remove 'translit_la_hy/tmp/train.tags.la-hy.tok.la': No such file or directory\n",
            "Tokenizer Version 1.1\n",
            "Language: la\n",
            "Number of threads: 8\n",
            "WARNING: No known abbreviations for language 'la', attempting fall-back to English version...\n",
            "rm: cannot remove 'translit_la_hy/tmp/train.tags.la-hy.tok.hy': No such file or directory\n",
            "Tokenizer Version 1.1\n",
            "Language: hy\n",
            "Number of threads: 8\n",
            "WARNING: No known abbreviations for language 'hy', attempting fall-back to English version...\n",
            "pre-processing test data...\n",
            "rm: cannot remove 'translit_la_hy/tmp/test.la': No such file or directory\n",
            "Tokenizer Version 1.1\n",
            "Language: la\n",
            "Number of threads: 8\n",
            "WARNING: No known abbreviations for language 'la', attempting fall-back to English version...\n",
            "\n",
            "rm: cannot remove 'translit_la_hy/tmp/test.hy': No such file or directory\n",
            "Tokenizer Version 1.1\n",
            "Language: hy\n",
            "Number of threads: 8\n",
            "WARNING: No known abbreviations for language 'hy', attempting fall-back to English version...\n",
            "\n",
            "splitting train and valid...\n",
            "learn_bpe.py on translit_la_hy/tmp/train.hy-la...\n",
            "subword-nmt/learn_bpe.py:267: DeprecationWarning: this script's location has moved to /content/fairseq/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "apply_bpe.py to train.la...\n",
            "subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='translit_la_hy/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "apply_bpe.py to valid.la...\n",
            "subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='translit_la_hy/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "apply_bpe.py to test.la...\n",
            "subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='translit_la_hy/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "apply_bpe.py to train.hy...\n",
            "subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='translit_la_hy/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "apply_bpe.py to valid.hy...\n",
            "subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='translit_la_hy/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "apply_bpe.py to test.hy...\n",
            "subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='translit_la_hy/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "clean-corpus.perl: processing translit_la_hy/tmp/bpe.train.la & .hy to translit_la_hy/train, cutoff 1-250, ratio 1.5\n",
            "..........(100000)..........(200000)..........(300000)..........(400000)..........(500000)..........(600000)..........(700000)..........(800000)..........(900000)........\n",
            "Input sentences: 980100  Output sentences:  979053\n",
            "clean-corpus.perl: processing translit_la_hy/tmp/bpe.valid.la & .hy to translit_la_hy/valid, cutoff 1-250, ratio 1.5\n",
            "\n",
            "Input sentences: 9900  Output sentences:  9892\n",
            "/content/fairseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eGVffTC7Srab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "a6b445ba-840b-4792-f3c2-892d2da7eb8c"
      },
      "cell_type": "code",
      "source": [
        "!python preprocess.py --source-lang la --target-lang hy \\\n",
        "  --trainpref examples/translation/translit_la_hy/train \\\n",
        "  --validpref examples/translation/translit_la_hy/valid \\\n",
        "  --testpref examples/translation/translit_la_hy/test \\\n",
        "  --destdir data-bin/translit_la_hy"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(alignfile=None, destdir='data-bin/translit_la_hy', joined_dictionary=False, nwordssrc=-1, nwordstgt=-1, only_source=False, output_format='binary', padding_factor=8, source_lang='la', srcdict=None, target_lang='hy', testpref='examples/translation/translit_la_hy/test', tgtdict=None, thresholdsrc=0, thresholdtgt=0, trainpref='examples/translation/translit_la_hy/train', validpref='examples/translation/translit_la_hy/valid')\n",
            "| [la] Dictionary: 1399 types\n",
            "| [la] examples/translation/translit_la_hy/train.la: 979053 sents, 41160824 tokens, 0.0% replaced by <unk>\n",
            "| [la] Dictionary: 1399 types\n",
            "| [la] examples/translation/translit_la_hy/valid.la: 9892 sents, 416323 tokens, 0.000961% replaced by <unk>\n",
            "| [la] Dictionary: 1399 types\n",
            "| [la] examples/translation/translit_la_hy/test.la: 10000 sents, 430508 tokens, 0.0446% replaced by <unk>\n",
            "| [hy] Dictionary: 1479 types\n",
            "| [hy] examples/translation/translit_la_hy/train.hy: 979053 sents, 41698210 tokens, 0.0% replaced by <unk>\n",
            "| [hy] Dictionary: 1479 types\n",
            "| [hy] examples/translation/translit_la_hy/valid.hy: 9892 sents, 422093 tokens, 0.000948% replaced by <unk>\n",
            "| [hy] Dictionary: 1479 types\n",
            "| [hy] examples/translation/translit_la_hy/test.hy: 10000 sents, 431585 tokens, 0.232% replaced by <unk>\n",
            "| Wrote preprocessed data to data-bin/translit_la_hy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AcNYbXT2O4Tc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "KZtPXjH5QJ__",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "a0ff0609-ea61-4c1f-861a-240a430ab9c7"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p checkpoints/fconv\n",
        "!CUDA_VISIBLE_DEVICES=0 python train.py data-bin/translit_la_hy \\\n",
        "  --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 132 \\\n",
        "  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "  --lr-scheduler fixed --force-anneal 200 \\\n",
        "  --arch fconv_iwslt_de_en --save-dir checkpoints/fconv \\\n",
        "  --skip-invalid-size-inputs-valid-test --max-epoch 10\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(arch='fconv_iwslt_de_en', clip_norm=0.1, criterion='label_smoothed_cross_entropy', data='data-bin/translit_la_hy', decoder_attention='True', decoder_embed_dim=256, decoder_embed_path=None, decoder_layers='[(256, 3)] * 3', decoder_out_embed_dim=256, device_id=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, encoder_embed_dim=256, encoder_embed_path=None, encoder_layers='[(256, 3)] * 4', force_anneal=200, fp16=False, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.25], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=10, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=132, max_update=0, min_loss_scale=0.0001, min_lr=1e-05, momentum=0.99, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, normalization_constant=0.5, optimizer='nag', raw_text=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/fconv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_input_output_embed=False, skip_invalid_size_inputs_valid_test=True, source_lang=None, target_lang=None, task='translation', train_subset='train', update_freq=[1], valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\r\n",
            "| [la] dictionary: 1400 types\r\n",
            "| [hy] dictionary: 1480 types\n",
            "| data-bin/translit_la_hy train 979053 examples\n",
            "| data-bin/translit_la_hy valid 9892 examples\n",
            "| model fconv_iwslt_de_en, criterion LabelSmoothedCrossEntropyCriterion\n",
            "| num. model params: 5063568\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = 132 and max sentences per GPU = None\n",
            "| epoch 001:   0%| | 1222/395961 [01:42<9:14:18, 11.87it/s, loss=3.440, nll_loss=2.184, ppl=4.54, wps=1240, ups=11.3, wpb=104, bsz=2, num_updates=1222, lr=0.25, gnorm=1.532, clip=100%, oom=0, wall=109]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SAFrx--fPeNr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ]
    },
    {
      "metadata": {
        "id": "V0DVOzfePiYs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python generate.py data-bin/translit.tokenized.latn-armn \\\n",
        "  --path checkpoints/fconv/checkpoint_best.pt \\\n",
        "  --batch-size 128 --beam 5 \\\n",
        "  --skip-invalid-size-inputs-valid-test"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
